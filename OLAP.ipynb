{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cccb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les valeurs manquantes de 'CaseID' ont été remplacées par -1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Charger le fichier CSV\n",
    "df = pd.read_csv('finaldata.csv')\n",
    "\n",
    "# 2️⃣ Remplacer les valeurs manquantes (NaN) de la colonne 'CaseID' par -1\n",
    "df['CaseID'] = df['CaseID'].fillna(-1)\n",
    "\n",
    "# 3️⃣ Sauvegarder le fichier CSV mis à jour\n",
    "df.to_csv('finaldata.csv', index=False)\n",
    "\n",
    "print(\"Les valeurs manquantes de 'CaseID' ont été remplacées par -1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b942c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le CSV\n",
    "df = pd.read_csv('finaldata.csv')\n",
    "\n",
    "# Remplir CaseID manquants\n",
    "df['CaseID'] = df['CaseID'].fillna(-1)\n",
    "\n",
    "# ----------------- DIMENSIONS -----------------\n",
    "dim_patient = df[['CaseID','Age','Sex']].drop_duplicates().reset_index(drop=True)\n",
    "dim_patient['PatientID'] = dim_patient.index + 1\n",
    "\n",
    "dim_diagnosis = df[['Diagnosis','Department','Commentary']].drop_duplicates().reset_index(drop=True)\n",
    "dim_diagnosis['DiagnosisID'] = dim_diagnosis.index + 1\n",
    "\n",
    "dim_date = df[['Date','DateTime']].drop_duplicates().reset_index(drop=True)\n",
    "dim_date['DateID'] = dim_date.index + 1\n",
    "\n",
    "dim_domaine = df[['domaine_medical']].drop_duplicates().reset_index(drop=True)\n",
    "dim_domaine['DomaineID'] = dim_domaine.index + 1\n",
    "\n",
    "dim_description = df[['Description','Anatomy','Chapter']].drop_duplicates().reset_index(drop=True)\n",
    "dim_description['DescriptionID'] = dim_description.index + 1\n",
    "\n",
    "dim_creation = df[['Creation']].drop_duplicates().reset_index(drop=True)\n",
    "dim_creation['CreationID'] = dim_creation.index + 1\n",
    "\n",
    "dim_author = df[['Author']].drop_duplicates().reset_index(drop=True)\n",
    "dim_author['AuthorID'] = dim_author.index + 1\n",
    "\n",
    "# ----------------- TABLE DE FAITS -----------------\n",
    "fact_table = df[['ID','State','Order','CaseID','Diagnosis','Date','domaine_medical','Description','Creation','Author']].copy()\n",
    "\n",
    "# Mapper les IDs\n",
    "fact_table['PatientID'] = fact_table['CaseID'].map(dict(zip(dim_patient['CaseID'], dim_patient['PatientID'])))\n",
    "fact_table['DiagnosisID'] = fact_table['Diagnosis'].map(dict(zip(dim_diagnosis['Diagnosis'], dim_diagnosis['DiagnosisID'])))\n",
    "fact_table['DateID'] = fact_table['Date'].map(dict(zip(dim_date['Date'], dim_date['DateID'])))\n",
    "fact_table['DomaineID'] = fact_table['domaine_medical'].map(dict(zip(dim_domaine['domaine_medical'], dim_domaine['DomaineID'])))\n",
    "fact_table['DescriptionID'] = fact_table['Description'].map(dict(zip(dim_description['Description'], dim_description['DescriptionID'])))\n",
    "fact_table['CreationID'] = fact_table['Creation'].map(dict(zip(dim_creation['Creation'], dim_creation['CreationID'])))\n",
    "fact_table['AuthorID'] = fact_table['Author'].map(dict(zip(dim_author['Author'], dim_author['AuthorID'])))\n",
    "\n",
    "# Garder seulement les IDs et les mesures\n",
    "fact_table_olap = fact_table[['ID','State','Order','PatientID','DiagnosisID','DateID','DomaineID','DescriptionID','CreationID','AuthorID']]\n",
    "\n",
    "# Sauvegarder\n",
    "fact_table_olap.to_csv('fact_rapport_medical.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082df15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tous les CSV des dimensions et de la table de faits ont été créés.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------- CHARGEMENT DU CSV -----------------\n",
    "df = pd.read_csv('finaldata.csv')\n",
    "\n",
    "# Remplir les valeurs manquantes de CaseID\n",
    "df['CaseID'] = df['CaseID'].fillna(-1)\n",
    "\n",
    "# ----------------- DIMENSIONS -----------------\n",
    "\n",
    "# dim_patient\n",
    "dim_patient = df[['CaseID','Age','Sex']].drop_duplicates().reset_index(drop=True)\n",
    "dim_patient['PatientID'] = dim_patient.index + 1\n",
    "dim_patient.to_csv('dim_patient.csv', index=False)\n",
    "\n",
    "# dimMedicalDiagnosis\n",
    "dim_diagnosis = df[['Diagnosis','Department','Commentary']].drop_duplicates().reset_index(drop=True)\n",
    "dim_diagnosis['DiagnosisID'] = dim_diagnosis.index + 1\n",
    "dim_diagnosis.to_csv('dim_diagnosis.csv', index=False)\n",
    "\n",
    "# dimDate\n",
    "dim_date = df[['Date','DateTime']].drop_duplicates().reset_index(drop=True)\n",
    "dim_date['DateID'] = dim_date.index + 1\n",
    "dim_date.to_csv('dim_date.csv', index=False)\n",
    "\n",
    "# dimDomaine\n",
    "dim_domaine = df[['domaine_medical']].drop_duplicates().reset_index(drop=True)\n",
    "dim_domaine['DomaineID'] = dim_domaine.index + 1\n",
    "dim_domaine.to_csv('dim_domaine.csv', index=False)\n",
    "\n",
    "# dimMedicalDescription\n",
    "dim_description = df[['Description','Anatomy','Chapter']].drop_duplicates().reset_index(drop=True)\n",
    "dim_description['DescriptionID'] = dim_description.index + 1\n",
    "dim_description.to_csv('dim_description.csv', index=False)\n",
    "\n",
    "# dimCreationDate\n",
    "dim_creation = df[['Creation']].drop_duplicates().reset_index(drop=True)\n",
    "dim_creation['CreationID'] = dim_creation.index + 1\n",
    "dim_creation.to_csv('dim_creation.csv', index=False)\n",
    "\n",
    "# dimAuthor\n",
    "dim_author = df[['Author']].drop_duplicates().reset_index(drop=True)\n",
    "dim_author['AuthorID'] = dim_author.index + 1\n",
    "dim_author.to_csv('dim_author.csv', index=False)\n",
    "\n",
    "# ----------------- TABLE DE FAITS -----------------\n",
    "# On mappe les IDs directement pour éviter merge massif\n",
    "fact_table = df[['ID','State','Order','CaseID','Diagnosis','Date','domaine_medical','Description','Creation','Author']].copy()\n",
    "\n",
    "fact_table['PatientID'] = fact_table['CaseID'].map(dict(zip(dim_patient['CaseID'], dim_patient['PatientID'])))\n",
    "fact_table['DiagnosisID'] = fact_table['Diagnosis'].map(dict(zip(dim_diagnosis['Diagnosis'], dim_diagnosis['DiagnosisID'])))\n",
    "fact_table['DateID'] = fact_table['Date'].map(dict(zip(dim_date['Date'], dim_date['DateID'])))\n",
    "fact_table['DomaineID'] = fact_table['domaine_medical'].map(dict(zip(dim_domaine['domaine_medical'], dim_domaine['DomaineID'])))\n",
    "fact_table['DescriptionID'] = fact_table['Description'].map(dict(zip(dim_description['Description'], dim_description['DescriptionID'])))\n",
    "fact_table['CreationID'] = fact_table['Creation'].map(dict(zip(dim_creation['Creation'], dim_creation['CreationID'])))\n",
    "fact_table['AuthorID'] = fact_table['Author'].map(dict(zip(dim_author['Author'], dim_author['AuthorID'])))\n",
    "\n",
    "# Garder seulement les clés étrangères et les mesures\n",
    "fact_table_olap = fact_table[['ID','State','Order','PatientID','DiagnosisID','DateID','DomaineID','DescriptionID','CreationID','AuthorID']]\n",
    "fact_table_olap.to_csv('fact_rapport_medical.csv', index=False)\n",
    "\n",
    "print(\"✅ Tous les CSV des dimensions et de la table de faits ont été créés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0856c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bough\\AppData\\Local\\Temp\\ipykernel_26932\\1224173367.py:20: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dim_date['DateTime'] = pd.to_datetime(dim_date['DateTime'], errors='coerce').dt.time\n",
      "C:\\Users\\bough\\AppData\\Local\\Temp\\ipykernel_26932\\1224173367.py:98: UserWarning: The provided table name 'dimMedicalDiagnosis' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  dim_diagnosis.to_sql('dimMedicalDiagnosis', con=engine, if_exists='append', index=False)\n",
      "C:\\Users\\bough\\AppData\\Local\\Temp\\ipykernel_26932\\1224173367.py:99: UserWarning: The provided table name 'dimDate' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  dim_date.to_sql('dimDate', con=engine, if_exists='append', index=False)\n",
      "C:\\Users\\bough\\AppData\\Local\\Temp\\ipykernel_26932\\1224173367.py:100: UserWarning: The provided table name 'dimDomaine' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  dim_domaine.to_sql('dimDomaine', con=engine, if_exists='append', index=False)\n",
      "C:\\Users\\bough\\AppData\\Local\\Temp\\ipykernel_26932\\1224173367.py:101: UserWarning: The provided table name 'dimMedicalDescription' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  dim_description.to_sql('dimMedicalDescription', con=engine, if_exists='append', index=False)\n",
      "C:\\Users\\bough\\AppData\\Local\\Temp\\ipykernel_26932\\1224173367.py:102: UserWarning: The provided table name 'dimCreationDate' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  dim_creation.to_sql('dimCreationDate', con=engine, if_exists='append', index=False)\n",
      "C:\\Users\\bough\\AppData\\Local\\Temp\\ipykernel_26932\\1224173367.py:103: UserWarning: The provided table name 'dimAuthor' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
      "  dim_author.to_sql('dimAuthor', con=engine, if_exists='append', index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Toutes les tables OLAP ont été créées et les données ont été chargées dans MySQL.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ----------------- CONFIGURATION MySQL -----------------\n",
    "# Remplace 'root' et 'mon_mdp' par ton utilisateur et mot de passe MySQL\n",
    "engine = create_engine(\"mysql+pymysql://root:@localhost/entrepot_medical\", echo=False)\n",
    "\n",
    "# ----------------- CHARGEMENT DES CSV -----------------\n",
    "dim_patient = pd.read_csv('dim_patient.csv')\n",
    "dim_diagnosis = pd.read_csv('dim_diagnosis.csv')\n",
    "dim_date = pd.read_csv('dim_date.csv')\n",
    "dim_domaine = pd.read_csv('dim_domaine.csv')\n",
    "dim_description = pd.read_csv('dim_description.csv')\n",
    "dim_creation = pd.read_csv('dim_creation.csv')\n",
    "dim_author = pd.read_csv('dim_author.csv')\n",
    "fact_table = pd.read_csv('fact_rapport_medical.csv')\n",
    "\n",
    "# ----------------- FORMAT DES DATES / HEURES -----------------\n",
    "dim_date['Date'] = pd.to_datetime(dim_date['Date'], format='%d.%m.%Y', errors='coerce').dt.date\n",
    "dim_date['DateTime'] = pd.to_datetime(dim_date['DateTime'], errors='coerce').dt.time\n",
    "dim_creation['Creation'] = pd.to_datetime(dim_creation['Creation'], format='%d.%m.%Y', errors='coerce').dt.date\n",
    "\n",
    "# ----------------- CREATION DES TABLES (avec FK) -----------------\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dim_patient (\n",
    "        PatientID INT PRIMARY KEY,\n",
    "        CaseID INT,\n",
    "        Age INT,\n",
    "        Sex VARCHAR(10)\n",
    "    )\"\"\"))\n",
    "\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dimMedicalDiagnosis (\n",
    "        DiagnosisID INT PRIMARY KEY,\n",
    "        Diagnosis VARCHAR(255),\n",
    "        Department VARCHAR(255),\n",
    "        Commentary TEXT\n",
    "    )\"\"\"))\n",
    "\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dimDate (\n",
    "        DateID INT PRIMARY KEY,\n",
    "        Date DATE,\n",
    "        DateTime TIME\n",
    "    )\"\"\"))\n",
    "\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dimDomaine (\n",
    "        DomaineID INT PRIMARY KEY,\n",
    "        domaine_medical VARCHAR(255)\n",
    "    )\"\"\"))\n",
    "\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dimMedicalDescription (\n",
    "        DescriptionID INT PRIMARY KEY,\n",
    "        Description TEXT,\n",
    "        Anatomy VARCHAR(255),\n",
    "        Chapter VARCHAR(255)\n",
    "    )\"\"\"))\n",
    "\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dimCreationDate (\n",
    "        CreationID INT PRIMARY KEY,\n",
    "        Creation DATE\n",
    "    )\"\"\"))\n",
    "\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dimAuthor (\n",
    "        AuthorID INT PRIMARY KEY,\n",
    "        Author VARCHAR(255)\n",
    "    )\"\"\"))\n",
    "\n",
    "    conn.execute(text(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fact_rapport_medical (\n",
    "        ID INT PRIMARY KEY,\n",
    "        State VARCHAR(50),\n",
    "        `Order` INT,\n",
    "        PatientID INT,\n",
    "        DiagnosisID INT,\n",
    "        DateID INT,\n",
    "        DomaineID INT,\n",
    "        DescriptionID INT,\n",
    "        CreationID INT,\n",
    "        AuthorID INT,\n",
    "        FOREIGN KEY (PatientID) REFERENCES dim_patient(PatientID),\n",
    "        FOREIGN KEY (DiagnosisID) REFERENCES dimMedicalDiagnosis(DiagnosisID),\n",
    "        FOREIGN KEY (DateID) REFERENCES dimDate(DateID),\n",
    "        FOREIGN KEY (DomaineID) REFERENCES dimDomaine(DomaineID),\n",
    "        FOREIGN KEY (DescriptionID) REFERENCES dimMedicalDescription(DescriptionID),\n",
    "        FOREIGN KEY (CreationID) REFERENCES dimCreationDate(CreationID),\n",
    "        FOREIGN KEY (AuthorID) REFERENCES dimAuthor(AuthorID)\n",
    "    )\"\"\"))\n",
    "\n",
    "# ----------------- INSERTION DES DONNÉES -----------------\n",
    "# On utilise 'append' pour conserver les FK et éviter de remplacer les tables\n",
    "dim_patient.to_sql('dim_patient', con=engine, if_exists='append', index=False)\n",
    "dim_diagnosis.to_sql('dimMedicalDiagnosis', con=engine, if_exists='append', index=False)\n",
    "dim_date.to_sql('dimDate', con=engine, if_exists='append', index=False)\n",
    "dim_domaine.to_sql('dimDomaine', con=engine, if_exists='append', index=False)\n",
    "dim_description.to_sql('dimMedicalDescription', con=engine, if_exists='append', index=False)\n",
    "dim_creation.to_sql('dimCreationDate', con=engine, if_exists='append', index=False)\n",
    "dim_author.to_sql('dimAuthor', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Charger la table de faits par chunks pour éviter MemoryError\n",
    "fact_table.to_sql('fact_rapport_medical', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "\n",
    "print(\"✅ Toutes les tables OLAP ont été créées et les données ont été chargées dans MySQL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f0219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
